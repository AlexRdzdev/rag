{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instalar las siguientes librerias:",
   "id": "d217b4eb9d882824"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -q transformers==4.41.2\n",
    "!pip install -q sentence-transformers==2.2.2\n",
    "!pip install -q chromadb==0.4.20"
   ],
   "id": "c7e015be24d3e49e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importamos pandas",
   "id": "1735ca7afca13ce2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pandas as pd",
   "id": "61c7c27b4dd53d50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Leemos el dataset utilizando Pandas, por cuestiones de no estresar demasiada memoria se utiliza un maximo de 1000 Registros",
   "id": "ad1ca6df17d2c7e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news = pd.read_csv('labelled_newscatcher_dataset.csv', sep=';')\n",
    "MAX_NEWS = 1000\n",
    "DOCUMENT=\"title\"\n",
    "TOPIC=\"topic\""
   ],
   "id": "653ebaa3632509c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news[\"id\"] = news.index\n",
    "news.head()\n",
    "subset_news = news.head(MAX_NEWS)"
   ],
   "id": "d17437443315f233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import chromadb",
   "id": "69d97c2261d485a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se inicializa el cliente de Chomadb apuntando al directorio actual",
   "id": "82c29e231e4ea81a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "chroma_client = chromadb.PersistentClient(path=\"\")",
   "id": "44ef3369f69afcd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from datetime import datetime",
   "id": "a86ccc1e1cf666f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se crea una colección de ChromaDB:",
   "id": "f66ac9d18f41bb17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "collection_name = \"news_collection_\" + str(int(datetime.now().timestamp()))\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "collection = chroma_client.create_collection(name=collection_name)"
   ],
   "id": "42bbdfe45d0500dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se agrega el subset de noticias a la coleccion de Chromadb",
   "id": "5a697f842befca1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "collection.add(\n",
    "    documents=subset_news[DOCUMENT].tolist(),\n",
    "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
    ")"
   ],
   "id": "e356d9d3f384fffa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Lista de preguntas y contextos\n",
    "questions = [\n",
    "    (\"technology\", \"What are the latest trends in technology advancements?\"),\n",
    "    (\"economy\", \"How has the economy been affected by recent global events?\"),\n",
    "    (\"climate\", \"What are the most recent developments in climate change research?\"),\n",
    "    (\"artificial intelligence\", \"What are the key findings in the field of AI this year?\"),\n",
    "    (\"politics\", \"How is politics influencing social media trends?\"),\n",
    "    (\"space\", \"What are the latest scientific discoveries in space exploration?\"),\n",
    "    (\"healthcare\", \"What are the emerging trends in the healthcare industry?\"),\n",
    "    (\"entertainment\", \"How is the entertainment industry adapting to new digital platforms?\"),\n",
    "    (\"education\", \"What are the biggest challenges facing the education sector today?\"),\n",
    "    (\"transportation\", \"What recent innovations are shaping the future of transportation?\")\n",
    "]"
   ],
   "id": "306b88378d22390d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm  # Para mostrar progreso\n",
    "\n",
    "# Definir los modelos a evaluar con sus tipos correspondientes\n",
    "models = {\n",
    "    \"dolly\": (\"databricks/dolly-v2-3b\", AutoModelForCausalLM),  # Dolly usa modelo causal\n",
    "    \"t5\": (\"t5-small\", AutoModelForSeq2SeqLM),  # T5 usa modelo seq2seq\n",
    "    \"tinyllama\": (\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", AutoModelForCausalLM)  # TinyLlama también es causal\n",
    "}\n",
    "\n",
    "# Nombre del archivo CSV para almacenar los resultados progresivamente\n",
    "output_csv = \"llm_model_comparison_con_contexto.csv\"\n",
    "\n",
    "# Inicializar el archivo CSV con encabezados\n",
    "df_init = pd.DataFrame(columns=[\"Model\", \"Topic\", \"Question\", \"Answer\", \"Execution Time (s)\",\n",
    "                                \"Memory Used (MB)\", \"Model Size (MB)\"])\n",
    "df_init.to_csv(output_csv, index=False)\n",
    "\n",
    "# Proceso de evaluación con barra de progreso\n",
    "for model_name, (model_path, model_class) in tqdm(models.items(), desc=\"Evaluando modelos\"):\n",
    "    print(f\"\\nCargando modelo: {model_name}\")\n",
    "\n",
    "    # Cargar modelo y tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = model_class.from_pretrained(model_path)\n",
    "    pipe = pipeline(\"text-generation\" if model_class == AutoModelForCausalLM else \"text2text-generation\",\n",
    "                    model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Calcular tamaño del modelo en MB\n",
    "    model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)\n",
    "\n",
    "    for topic, question in tqdm(questions, desc=f\"Procesando preguntas con {model_name}\", leave=False):\n",
    "        results = collection.query(query_texts=[topic], n_results=10)\n",
    "        context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "\n",
    "        prompt_template = f\"\"\"\n",
    "        Relevant context: {context}\n",
    "        Considering the relevant context, answer the question.\n",
    "        Question: {question}\n",
    "        Answer: \"\"\"\n",
    "\n",
    "        # Medición del uso de memoria antes\n",
    "        mem_before = psutil.virtual_memory().used / (1024 ** 2)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generar respuesta del modelo con max_new_tokens\n",
    "        try:\n",
    "            lm_response = pipe(prompt_template, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "            answer = lm_response[0]['generated_text'] if lm_response else \"No response\"\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "\n",
    "        print(answer)\n",
    "\n",
    "        end_time = time.time()\n",
    "        mem_after = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "        # Calcular métricas\n",
    "        execution_time = end_time - start_time\n",
    "        memory_used = mem_after - mem_before\n",
    "\n",
    "        # Almacenar resultados en un diccionario\n",
    "        result_data = {\n",
    "            \"Model\": model_name,\n",
    "            \"Topic\": topic,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Execution Time (s)\": execution_time,\n",
    "            \"Memory Used (MB)\": memory_used,\n",
    "            \"Model Size (MB)\": model_size\n",
    "        }\n",
    "\n",
    "        # Guardar el resultado en el archivo CSV inmediatamente\n",
    "        df_temp = pd.DataFrame([result_data])\n",
    "        df_temp.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Guardado: Modelo {model_name}, Tópico {topic}\")\n",
    "\n",
    "print(f\"Todas las métricas han sido guardadas en '{output_csv}'. Puedes revisarlas en cualquier momento.\")\n"
   ],
   "id": "2e11a1559df08d19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm  # Para mostrar progreso\n",
    "\n",
    "# Definir los modelos a evaluar con sus tipos correspondientes\n",
    "models = {\n",
    "    \"dolly\": (\"databricks/dolly-v2-3b\", AutoModelForCausalLM),  # Dolly usa modelo causal\n",
    "    \"t5\": (\"t5-small\", AutoModelForSeq2SeqLM),  # T5 usa modelo seq2seq\n",
    "    \"tinyllama\": (\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", AutoModelForCausalLM)  # TinyLlama también es causal\n",
    "}\n",
    "\n",
    "# Nombre del archivo CSV para almacenar los resultados progresivamente\n",
    "output_csv = \"llm_model_comparison_sin_contexto.csv\"\n",
    "\n",
    "# Inicializar el archivo CSV con encabezados\n",
    "df_init = pd.DataFrame(columns=[\"Model\", \"Topic\", \"Question\", \"Answer\", \"Execution Time (s)\",\n",
    "                                \"Memory Used (MB)\", \"Model Size (MB)\"])\n",
    "df_init.to_csv(output_csv, index=False)\n",
    "\n",
    "# Proceso de evaluación con barra de progreso\n",
    "for model_name, (model_path, model_class) in tqdm(models.items(), desc=\"Evaluando modelos\"):\n",
    "    print(f\"\\nCargando modelo: {model_name}\")\n",
    "\n",
    "    # Cargar modelo y tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = model_class.from_pretrained(model_path)\n",
    "    pipe = pipeline(\"text-generation\" if model_class == AutoModelForCausalLM else \"text2text-generation\",\n",
    "                    model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Calcular tamaño del modelo en MB\n",
    "    model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)\n",
    "\n",
    "    for topic, question in tqdm(questions, desc=f\"Procesando preguntas con {model_name}\", leave=False):\n",
    "\n",
    "        prompt_template = f\"\"\"\n",
    "        answer the question.\n",
    "        Question: {question}\n",
    "        Answer: \"\"\"\n",
    "\n",
    "        # Medición del uso de memoria antes\n",
    "        mem_before = psutil.virtual_memory().used / (1024 ** 2)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generar respuesta del modelo con max_new_tokens\n",
    "        try:\n",
    "            lm_response = pipe(prompt_template, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "            answer = lm_response[0]['generated_text'] if lm_response else \"No response\"\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "\n",
    "        print(answer)\n",
    "\n",
    "        end_time = time.time()\n",
    "        mem_after = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "        # Calcular métricas\n",
    "        execution_time = end_time - start_time\n",
    "        memory_used = mem_after - mem_before\n",
    "\n",
    "        # Almacenar resultados en un diccionario\n",
    "        result_data = {\n",
    "            \"Model\": model_name,\n",
    "            \"Topic\": topic,\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Execution Time (s)\": execution_time,\n",
    "            \"Memory Used (MB)\": memory_used,\n",
    "            \"Model Size (MB)\": model_size\n",
    "        }\n",
    "\n",
    "        # Guardar el resultado en el archivo CSV inmediatamente\n",
    "        df_temp = pd.DataFrame([result_data])\n",
    "        df_temp.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Guardado: Modelo {model_name}, Tópico {topic}\")\n",
    "\n",
    "print(f\"Todas las métricas han sido guardadas en '{output_csv}'. Puedes revisarlas en cualquier momento.\")\n"
   ],
   "id": "c132ea2e9d72824c"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
